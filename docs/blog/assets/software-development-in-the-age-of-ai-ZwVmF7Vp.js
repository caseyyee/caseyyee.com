import{u as r,j as n}from"./index-DhxtBCdR.js";const t={layout:"minimal",title:"The Road to AI",description:"How I'm navigate my way through learing and applying new AI technologies to my work.",date:"2025-02-18",showSidebar:!1};function l(i){const e={a:"a",div:"div",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...r(),...i.components};return n.jsxs(n.Fragment,{children:[n.jsx(e.header,{children:n.jsxs(e.h1,{id:"exploration",children:["Exploration",n.jsx(e.a,{"aria-hidden":"true",tabIndex:"-1",href:"#exploration",children:n.jsx(e.div,{"data-autolink-icon":!0})})]})}),`
`,n.jsx(e.p,{children:"This is a fast moving space. Just writing down all my notes as I go."}),`
`,n.jsxs(e.h3,{id:"coding",children:["Coding",n.jsx(e.a,{"aria-hidden":"true",tabIndex:"-1",href:"#coding",children:n.jsx(e.div,{"data-autolink-icon":!0})})]}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsxs(e.li,{children:[`
`,n.jsx(e.p,{children:n.jsx(e.a,{href:"https://www.cursor.com/",children:"Cursor"})}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsx(e.li,{children:".cursor/rules"}),`
`]}),`
`]}),`
`,n.jsxs(e.li,{children:[`
`,n.jsx(e.p,{children:"Run LLMs locally"}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsx(e.li,{children:n.jsx(e.a,{href:"https://ollama.com/library/deepseek-r1",children:"Ollama"})}),`
`]}),`
`]}),`
`]}),`
`,n.jsxs(e.h2,{id:"running-llm-locally",children:["Running LLM locally",n.jsx(e.a,{"aria-hidden":"true",tabIndex:"-1",href:"#running-llm-locally",children:n.jsx(e.div,{"data-autolink-icon":!0})})]}),`
`,n.jsx(e.p,{children:`Running the Deepseek R1 model, 8b parameters. Performs OK on my M2 air. Good enough for small experiments. Would really like to look at other "offline" options that don't depend on services. I think future functionality using small LLM's on mobile devices or inside the browser might be an interesting space for offline cases, low power devices, or in situations where you want privacy, autonomy or security.`}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsx(e.li,{children:n.jsx(e.a,{href:"https://webllm.mlc.ai/",children:"WebLLM"})}),`
`,n.jsx(e.li,{children:n.jsx(e.a,{href:"https://lmstudio.ai/",children:"LM Studio"})}),`
`]}),`
`,n.jsxs(e.h2,{id:"agents",children:["Agents",n.jsx(e.a,{"aria-hidden":"true",tabIndex:"-1",href:"#agents",children:n.jsx(e.div,{"data-autolink-icon":!0})})]}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsxs(e.li,{children:[`
`,n.jsx(e.p,{children:n.jsx(e.a,{href:"https://gooseai.com/",children:"Goose AI agent"})}),`
`]}),`
`,n.jsxs(e.li,{children:[`
`,n.jsx(e.p,{children:n.jsx(e.a,{href:"https://sdk.vercel.ai/",children:"Vercel AI SDK"})}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsx(e.li,{children:"Ability to quickly create web. Multi-model capabilities."}),`
`]}),`
`]}),`
`,n.jsxs(e.li,{children:[`
`,n.jsx(e.p,{children:n.jsx(e.a,{href:"https://github.com/modelcontextprotocol",children:"Model context Protocol - Anthropic"})}),`
`,n.jsxs(e.ul,{children:[`
`,n.jsx(e.li,{children:"Conceptually easier to understand than Langchain."}),`
`,n.jsx(e.li,{children:"Client/Server model is a interesting concept that allows for interoptability between services and LLM's."}),`
`,n.jsx(e.li,{children:"Typescript"}),`
`]}),`
`]}),`
`]})]})}function o(i={}){const{wrapper:e}={...r(),...i.components};return e?n.jsx(e,{...i,children:n.jsx(l,{...i})}):l(i)}export{o as default,t as frontmatter};
